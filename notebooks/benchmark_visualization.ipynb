{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cloud LLM Inference Benchmark - Results Visualization\n",
        "\n",
        "**Course:** MSML 650 - Cloud Computing  \n",
        "**Project:** Cloud Deployment of Model Serving Platforms: Benchmarking vLLM and SGLang\n",
        "\n",
        "This notebook visualizes and analyzes the benchmark results comparing vLLM and SGLang inference frameworks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure display\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Benchmark Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define results directory\n",
        "RESULTS_DIR = Path(\"../results\")\n",
        "\n",
        "def load_results(framework: str) -> list:\n",
        "    \"\"\"Load all benchmark results for a framework\"\"\"\n",
        "    results = []\n",
        "    framework_dir = RESULTS_DIR / framework\n",
        "    \n",
        "    if framework_dir.exists():\n",
        "        for result_file in framework_dir.glob(\"*.json\"):\n",
        "            try:\n",
        "                with open(result_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    data['source_file'] = result_file.name\n",
        "                    results.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {result_file}: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Load results\n",
        "vllm_results = load_results('vllm')\n",
        "sglang_results = load_results('sglang')\n",
        "\n",
        "print(f\"Loaded {len(vllm_results)} vLLM results\")\n",
        "print(f\"Loaded {len(sglang_results)} SGLang results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrames\n",
        "def results_to_dataframe(results: list, framework: str) -> pd.DataFrame:\n",
        "    \"\"\"Convert results list to DataFrame\"\"\"\n",
        "    if not results:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    df_data = []\n",
        "    for r in results:\n",
        "        df_data.append({\n",
        "            'framework': framework,\n",
        "            'workload': r.get('workload_name', 'unknown'),\n",
        "            'throughput_tps': r.get('throughput_tps', 0),\n",
        "            'throughput_rps': r.get('throughput_rps', 0),\n",
        "            'avg_latency': r.get('avg_latency', 0),\n",
        "            'min_latency': r.get('min_latency', 0),\n",
        "            'max_latency': r.get('max_latency', 0),\n",
        "            'p50_latency': r.get('p50_latency', 0),\n",
        "            'p90_latency': r.get('p90_latency', 0),\n",
        "            'p95_latency': r.get('p95_latency', 0),\n",
        "            'p99_latency': r.get('p99_latency', 0),\n",
        "            'total_requests': r.get('total_requests', 0),\n",
        "            'successful_requests': r.get('successful_requests', 0),\n",
        "            'failed_requests': r.get('failed_requests', 0),\n",
        "            'total_time': r.get('total_time', 0),\n",
        "            'total_output_tokens': r.get('total_output_tokens', 0),\n",
        "            'timestamp': r.get('timestamp', '')\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(df_data)\n",
        "\n",
        "# Create DataFrames\n",
        "df_vllm = results_to_dataframe(vllm_results, 'vLLM')\n",
        "df_sglang = results_to_dataframe(sglang_results, 'SGLang')\n",
        "\n",
        "# Combine\n",
        "df_all = pd.concat([df_vllm, df_sglang], ignore_index=True)\n",
        "\n",
        "# Calculate success rate\n",
        "if not df_all.empty:\n",
        "    df_all['success_rate'] = (df_all['successful_requests'] / df_all['total_requests'] * 100).fillna(0)\n",
        "\n",
        "print(\"Data loaded and processed!\")\n",
        "df_all.head() if not df_all.empty else print(\"No data available yet - run benchmarks first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparing throughput\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "if not df_all.empty:\n",
        "    # Throughput TPS\n",
        "    ax1 = sns.barplot(data=df_all, x='framework', y='throughput_tps', ax=axes[0], palette='Set2')\n",
        "    axes[0].set_title('Average Throughput (Tokens per Second)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Framework')\n",
        "    axes[0].set_ylabel('Tokens/Second')\n",
        "    \n",
        "    # Add value labels\n",
        "    for p in ax1.patches:\n",
        "        ax1.annotate(f'{p.get_height():.2f}', \n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                    ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "    # Throughput RPS\n",
        "    ax2 = sns.barplot(data=df_all, x='framework', y='throughput_rps', ax=axes[1], palette='Set2')\n",
        "    axes[1].set_title('Average Throughput (Requests per Second)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Framework')\n",
        "    axes[1].set_ylabel('Requests/Second')\n",
        "    \n",
        "    for p in ax2.patches:\n",
        "        ax2.annotate(f'{p.get_height():.2f}', \n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                    ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/throughput_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Throughput comparison saved to results/throughput_comparison.png\")\n",
        "else:\n",
        "    print(\"No data available - run benchmarks first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Latency Distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cost Efficiency Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define GPU instance costs (USD per hour)\n",
        "GPU_COSTS = {\n",
        "    'g4dn.xlarge': 0.526,   # NVIDIA T4\n",
        "    'g5.xlarge': 1.006,     # NVIDIA A10G\n",
        "    'g5.2xlarge': 1.212,    # NVIDIA A10G\n",
        "    'p3.2xlarge': 3.06,     # NVIDIA V100\n",
        "    'p4d.24xlarge': 32.77,  # NVIDIA A100\n",
        "}\n",
        "\n",
        "# Calculate cost per million tokens\n",
        "def calculate_cost_per_million_tokens(throughput_tps, hourly_cost):\n",
        "    \"\"\"Calculate cost per million tokens generated\"\"\"\n",
        "    if throughput_tps <= 0:\n",
        "        return float('inf')\n",
        "    tokens_per_hour = throughput_tps * 3600\n",
        "    cost_per_token = hourly_cost / tokens_per_hour\n",
        "    cost_per_million = cost_per_token * 1_000_000\n",
        "    return cost_per_million\n",
        "\n",
        "# Calculate for g5.xlarge (common choice)\n",
        "instance_type = 'g5.xlarge'\n",
        "hourly_cost = GPU_COSTS[instance_type]\n",
        "\n",
        "if not df_all.empty:\n",
        "    cost_data = df_all.groupby('framework')['throughput_tps'].mean().reset_index()\n",
        "    cost_data['cost_per_million_tokens'] = cost_data['throughput_tps'].apply(\n",
        "        lambda x: calculate_cost_per_million_tokens(x, hourly_cost)\n",
        "    )\n",
        "    cost_data['instance_type'] = instance_type\n",
        "    cost_data['hourly_cost'] = hourly_cost\n",
        "    \n",
        "    print(f\"\\nCost Analysis ({instance_type} @ ${hourly_cost}/hr):\")\n",
        "    print(cost_data[['framework', 'throughput_tps', 'cost_per_million_tokens']].to_string(index=False))\n",
        "    \n",
        "    # Visualize\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    bars = ax.bar(cost_data['framework'], cost_data['cost_per_million_tokens'], color=['#2ecc71', '#3498db'])\n",
        "    ax.set_title(f'Cost per Million Tokens ({instance_type})', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Framework')\n",
        "    ax.set_ylabel('Cost (USD)')\n",
        "    \n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'${height:.2f}',\n",
        "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                   xytext=(0, 3), textcoords=\"offset points\",\n",
        "                   ha='center', va='bottom', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/cost_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data available - run benchmarks first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "if not df_all.empty:\n",
        "    print(\"=\"*60)\n",
        "    print(\"BENCHMARK SUMMARY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for framework in df_all['framework'].unique():\n",
        "        fw_data = df_all[df_all['framework'] == framework]\n",
        "        \n",
        "        print(f\"\\n{framework}\")\n",
        "        print(\"-\"*40)\n",
        "        print(f\"  Number of benchmarks: {len(fw_data)}\")\n",
        "        print(f\"  Avg Throughput (TPS): {fw_data['throughput_tps'].mean():.2f}\")\n",
        "        print(f\"  Avg Latency: {fw_data['avg_latency'].mean():.3f}s\")\n",
        "        print(f\"  P95 Latency: {fw_data['p95_latency'].mean():.3f}s\")\n",
        "        print(f\"  Avg Success Rate: {fw_data['success_rate'].mean():.1f}%\")\n",
        "        print(f\"  Total Tokens Generated: {fw_data['total_output_tokens'].sum():,}\")\n",
        "    \n",
        "    # Determine winner\n",
        "    if len(df_all['framework'].unique()) == 2:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CONCLUSION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        vllm_tps = df_all[df_all['framework'] == 'vLLM']['throughput_tps'].mean()\n",
        "        sglang_tps = df_all[df_all['framework'] == 'SGLang']['throughput_tps'].mean()\n",
        "        \n",
        "        if vllm_tps > sglang_tps:\n",
        "            diff = ((vllm_tps - sglang_tps) / sglang_tps) * 100 if sglang_tps > 0 else 0\n",
        "            print(f\"\\n✓ vLLM outperforms SGLang by {diff:.1f}% in throughput\")\n",
        "        else:\n",
        "            diff = ((sglang_tps - vllm_tps) / vllm_tps) * 100 if vllm_tps > 0 else 0\n",
        "            print(f\"\\n✓ SGLang outperforms vLLM by {diff:.1f}% in throughput\")\n",
        "    \n",
        "    # Save to CSV\n",
        "    df_all.to_csv('../results/benchmark_results_combined.csv', index=False)\n",
        "    print(\"\\nAll results saved to results/benchmark_results_combined.csv\")\n",
        "else:\n",
        "    print(\"No benchmark data available yet. Run benchmarks first!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latency percentiles comparison\n",
        "if not df_all.empty:\n",
        "    latency_cols = ['avg_latency', 'p50_latency', 'p90_latency', 'p95_latency', 'p99_latency']\n",
        "    latency_data = df_all.groupby('framework')[latency_cols].mean().T\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    x = np.arange(len(latency_cols))\n",
        "    width = 0.35\n",
        "    \n",
        "    if 'vLLM' in latency_data.columns:\n",
        "        bars1 = ax.bar(x - width/2, latency_data['vLLM'], width, label='vLLM', color='#2ecc71')\n",
        "    if 'SGLang' in latency_data.columns:\n",
        "        bars2 = ax.bar(x + width/2, latency_data['SGLang'], width, label='SGLang', color='#3498db')\n",
        "    \n",
        "    ax.set_xlabel('Latency Metric')\n",
        "    ax.set_ylabel('Latency (seconds)')\n",
        "    ax.set_title('Latency Distribution Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(['Average', 'P50', 'P90', 'P95', 'P99'])\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/latency_distribution.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Latency distribution saved to results/latency_distribution.png\")\n",
        "else:\n",
        "    print(\"No data available - run benchmarks first!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
